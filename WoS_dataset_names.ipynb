{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17db3d59-08ae-40e0-ae38-7491101518e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import glob\n",
    "# import timeit\n",
    "# import time\n",
    "# from multiprocessing import Pool\n",
    "# import numpy as np\n",
    "# def toc(start_time):\n",
    "#     elapsed = timeit.default_timer() - start_time\n",
    "#     print(elapsed)\n",
    "   \n",
    "    \n",
    "    \n",
    "# # # # \n",
    "# # start_time = timeit.default_timer()\n",
    "# # names=\"/mnt/sdb1/sandeep/article_c5_ALL.csv.gz\"\n",
    "# # files=sorted(glob.glob(names))\n",
    "\n",
    "# # p=Pool(processes=10)\n",
    "# # df_c5 = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "# # print(toc(start_time))\n",
    "# # # names_par=\"/mnt/sdb1/sandeep/z_article_c5.parquet\"\n",
    "# # # df_c5.to_parquet(names_par,index=None)\n",
    "\n",
    "# # p.close()\n",
    "# # print(toc(start_time))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# start_time = timeit.default_timer()\n",
    "\n",
    "# # ----------------------------------------------------------------------------------\n",
    "# # Loading auth info files in batches of 10\n",
    "# # out of all the 5000 files, NNN=500\n",
    "# # ----------------------------------------------------------------------------------\n",
    "# t_ic = time.time();\n",
    "# for i_parquet in range(0,5):    \n",
    "#     names_i=\"/mnt/sdb1/sandeep/upward_mobi/auth_info_with_GEO00000000\"+str(i_parquet)\n",
    "# # ----------------------------------------------------------------------------------\n",
    "#     for j_parquet in range(0,10):\n",
    "#         names_ij=names_i+str(j_parquet)        \n",
    "# # ----------------------------------------------------------------------------------\n",
    "#         for k_parquet in range(0,10):  \n",
    "#             t_oc = time.time();\n",
    "#             with open(\"/mnt/sdb1/sandeep/Career Transitions/000_progress_bar_READ_upward_mobi.txt\", \"a\") as file_object:\n",
    "#                 file_object.write(str(i_parquet)+str(j_parquet)+str(k_parquet)+'---time taken-'+str(round(t_oc-t_ic,2))+'\\n')\n",
    "\n",
    "#             p=Pool(processes=10)\n",
    "\n",
    "#             names=names_ij+str(k_parquet)+'*'\n",
    "#             names_par0=\"/mnt/sdb1/sandeep/upward_mobi_parquets/dais_auth_info\"+str(i_parquet)+str(j_parquet)+str(k_parquet)+\".parquet\"\n",
    "#             names_par1=\"/mnt/sdb1/sandeep/upward_mobi_parquets/name_auth_info\"+str(i_parquet)+str(j_parquet)+str(k_parquet)+\".parquet\"\n",
    "\n",
    "#             start_time = timeit.default_timer()\n",
    "#             print(str(i_parquet)+str(j_parquet)+str(k_parquet))\n",
    "#             files=sorted(glob.glob(names))\n",
    "#             df_temp = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "#             print(toc(start_time))\n",
    "#             # ----------------------------------------------------------------------------------\n",
    "#             # dropping NAN rows and sorting the table auth info\n",
    "#             # ----------------------------------------------------------------------------------\n",
    "\n",
    "#             df_temp=df_temp.dropna(subset=['article_id', 'auth_dais'], how='any')\n",
    "\n",
    "\n",
    "# #             df_temp_sorted=df_temp.sort_values(\"auth_dais\")\n",
    "#             # ----------------------------------------------------------------------------------\n",
    "# #             AUTH_IDS=list(df_temp_sorted.auth_dais)\n",
    "#             # ----------------------------------------------------------------------------------\n",
    "#             # Matching author ids in author_listX X=20, to filter out authors with less than 20 papers\n",
    "#             # ----------------------------------------------------------------------------------\n",
    "#             J=df_temp.shape[0]\n",
    "\n",
    "#             df1=df_temp.iloc[0:J,:].copy()\n",
    "#             df1['affil_geo']= df1.org.fillna('') +', '+ df1.city.fillna('') +', '+ df1.state.fillna('') +', '+ df1.country.fillna('') +', '+ df1.postal.fillna('')\n",
    "#             del df1['org'],df1['city'],df1['sub_org'],df1['state'],df1['country'],df1['postal']\n",
    "            \n",
    "# #             df2=df_paper_counts.copy()\n",
    "# #             df2.set_index(\"dais\", inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "# #             merged_left=df1.merge(df2.loc[df2.index.isin(df1['auth_dais'])],\n",
    "# #               how='left', left_on='auth_dais', right_index=True) \n",
    "\n",
    "            \n",
    "# #             df_author=merged_left.loc[merged_left['num_papers'] > NUM_PAPER_THRESHOLD]\n",
    "# #             df_author.head()\n",
    "# #             # ----------------------------------------------------------------------------------\n",
    "# #             # Separating columns org and name which are objects and take a lot of space\n",
    "# #             # ----------------------------------------------------------------------------------\n",
    "\n",
    "# #             df_author_no_name=df_author.iloc[:,[0,1,3,4]]\n",
    "# #             df_author_name=df_author.iloc[:,[1,2]]\n",
    "# #             # ----------------------------------------------------------------------------------\n",
    "# #             #SAVING TABLES AS parquets\n",
    "# #             # ----------------------------------------------------------------------------------\n",
    "# #             print('----merged----',toc(start_time))\n",
    "            \n",
    "\n",
    "#             print('----finished writing to file---',toc(start_time))\n",
    "#             p.close()\n",
    "#             break\n",
    "#         break\n",
    "#     break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d69ac063-ade6-4093-9795-9bc11a17b206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Valencia, R.', 'Bueno-de-Mesquita, H. Bas', 'Elbaz, D.', ...,\n",
       "       'Freeman, Laura Beane', 'Yu, Xinbing', 'He, Ping'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.auth_name.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c388a6ca-ad5a-45b1-844e-7cfc9f45a0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586e8f6-1c28-44a5-be04-dfe29e42ca81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
